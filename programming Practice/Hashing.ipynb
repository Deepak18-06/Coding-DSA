{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Sum Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Sum Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Longest Consecutive Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def longestConsecutive(nums):\n",
    "    longest_streak = 0\n",
    "    num_set = set(nums)\n",
    "    for i in num_set:\n",
    "        curr_num = i\n",
    "        curr_streak = 1\n",
    "        while curr_num +1 in num_set:\n",
    "            curr_num+=1\n",
    "            curr_streak+=1\n",
    "        longest_streak = max(longest_streak,curr_streak)\n",
    "    return longest_streak\n",
    "arr = [0,3,7,2,5,8,4,6,0,1]\n",
    "longestConsecutive(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Largest subarray with 0 sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def zeroSumLen(arr):\n",
    "    HashMap = dict()\n",
    "    result = 0\n",
    "    sum_now = 0\n",
    "    for i in range(len(arr)):\n",
    "        sum_now+=arr[i]\n",
    "        if arr[i] == 0 and result == 0:\n",
    "            result = 1\n",
    "        if sum_now == 0:\n",
    "            result = i+1\n",
    "        elif sum_now in HashMap:\n",
    "            result = max(result,i-HashMap[sum_now])\n",
    "        elif sum_now not in HashMap:\n",
    "            HashMap[sum_now] = i\n",
    "    return result\n",
    "arr = [15, -2, 2, -8, 1, 7, 10, 13] \n",
    "zeroSumLen(arr) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-7-ead0186fe434>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-ead0186fe434>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    from nltk.tokenize import WhitespaceTokenizer\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.tokenize import\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-6-0e265953e104>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-0e265953e104>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    from nltk.tokenize import WhitespaceTokenizer\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "paragraph=\"\"\"This is nachiketa's car, doesn't seem like small car? Hi how are you. I am fine.\"\"\"\n",
    "sentnce=nltk.sent_tokenize(paragraph);\n",
    "print(sentnce[0])\n",
    "\n",
    "text=sentnce[0]\n",
    "tk=WhitespaceTokenizer();\n",
    "output=tk.tokenize(text)\n",
    "2-print(output)\n",
    "3-print(len(output))\n",
    "\n",
    "tk=WordPunctTokenizer();\n",
    "output1=tk.tokenize(text)\n",
    "4-print(output1)\n",
    "5-print(len(output1))\n",
    "\n",
    "tk=TreebankWordTokenizer();\n",
    "output2=tk.tokenize(text);\n",
    "6-print(output2)\n",
    "7-print(len(output2))\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# Stemming\n",
    "for i in range(len(paragraph)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i] = ' '.join(words) \n",
    "\n",
    "8-print(sentences) ;\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# lemmatize\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i] = ' '.join(words) \n",
    "9-print(sentences) ;\n",
    "\n",
    "text = nltk.word_tokenize(\"Ram is going. He is good boy\")\n",
    "10-print(nltk.pos_tag(text)) ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
